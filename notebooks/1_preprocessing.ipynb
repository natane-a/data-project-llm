{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V972riVQWQf-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code just for illustrating steps. To run model use summarization_llm.ipynb\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers torch accelerate evaluate"
      ],
      "metadata": {
        "id": "2FEwu8Z_WSMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "a2dHFY7qWSOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('multi_news', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "L8qUaa-6WSQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "qQoB6PMsWSXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0]"
      ],
      "metadata": {
        "id": "kamlvqX5WSaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'].features"
      ],
      "metadata": {
        "id": "WsaC02KJWk5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_docs = len(ds['train'])\n",
        "num_test_docs = len(ds['test'])\n",
        "print(f\"Number of documents in training set: {num_train_docs}\")\n",
        "print(f\"Number of documents in testing set: {num_test_docs}\")"
      ],
      "metadata": {
        "id": "5_i4OtOmWlAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the subset data\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"document\"]\n",
        "    targets = examples[\"summary\"]\n",
        "\n",
        "    # tokenize the document, truncating at the model's max input size (1024 tokens)\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # tokenize the summary, setting a max_length to correspond to 250 tokens\n",
        "    labels = tokenizer(targets, max_length=250, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# apply preprocessing to the subsets (batched for efficiency)\n",
        "tokenized_train_subset = train_subset.map(preprocess_function, batched=True)\n",
        "tokenized_test_subset = test_subset.map(preprocess_function, batched=True)\n",
        "\n",
        "train_dataset = tokenized_train_subset"
      ],
      "metadata": {
        "id": "FFKt6CZ-WlQh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}