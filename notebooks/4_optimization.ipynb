{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP0NAZoiW8Gq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code just for illustrating steps. To run model use summarization_llm.ipynb\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "repo_name = \"llm-summarization-project\"\n",
        "\n",
        "# load the model\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# define training arguments with fewer epochs for faster training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=repo_name,                  # directory to save checkpoints\n",
        "    evaluation_strategy=\"epoch\",           # evaluate at the end of each epoch\n",
        "    learning_rate=2e-5,                    # kearning rate\n",
        "    per_device_train_batch_size=4,         # training batch size\n",
        "    per_device_eval_batch_size=4,          # evaluation batch size\n",
        "    num_train_epochs=1,                    # 1 epoch for quicker training\n",
        "    fp16=True,                             # enable mixed precision for faster training on GPUs\n",
        "    gradient_accumulation_steps=2,\n",
        "    push_to_hub=True,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",\n",
        "    seed=42,                               # random seed set for repoducibility\n",
        ")\n",
        "\n",
        "# initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"bart_large_cnn_finetuned\")\n",
        "\n",
        "# evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "LUuDbOUsW8pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "pqPIwl1uXB0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git-lfs"
      ],
      "metadata": {
        "id": "F-L05wVmXEJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "repo_name = \"llm-summarization-project\"\n",
        "\n",
        "# load the model\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# define training arguments with fewer epochs for faster training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=repo_name,                  # directory to save checkpoints\n",
        "    evaluation_strategy=\"epoch\",           # evaluate at the end of each epoch\n",
        "    learning_rate=2e-5,                    # kearning rate\n",
        "    per_device_train_batch_size=4,         # training batch size\n",
        "    per_device_eval_batch_size=4,          # evaluation batch size\n",
        "    num_train_epochs=1,                    # 1 epoch for quicker training\n",
        "    fp16=True,                             # enable mixed precision for faster training on GPUs\n",
        "    gradient_accumulation_steps=2,\n",
        "    push_to_hub=True,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",\n",
        "    seed=42,                               # random seed set for repoducibility\n",
        ")\n",
        "\n",
        "# initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"bart_large_cnn_finetuned\")\n",
        "\n",
        "# evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "8FLfJX02XELh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "J6kmCsIyXJFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Testing Model\n",
        "'''\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"natanea/llm-summarization-project\")\n",
        "\n",
        "# example document\n",
        "document = ds['train'][0]['document']\n",
        "\n",
        "# generate summary\n",
        "summary = summarizer(document, max_length=130, min_length=30, do_sample=False)\n",
        "print(\"Summary:\", summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "GnquUNsdXKp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}